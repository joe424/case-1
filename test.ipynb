{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"test.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"F4d3iBKUdFxe"},"source":["# ref: https://www.analyticsvidhya.com/blog/2020/01/first-text-classification-in-pytorch/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KfHlurxpdixu","executionInfo":{"status":"ok","timestamp":1602552282965,"user_tz":-480,"elapsed":809,"user":{"displayName":"張哲魁","photoUrl":"","userId":"09477290535038822979"}},"outputId":"0edd8525-8835-4382-b6bf-c7b89247f440","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PFudKDotd6Ub","executionInfo":{"status":"ok","timestamp":1602552282966,"user_tz":-480,"elapsed":801,"user":{"displayName":"張哲魁","photoUrl":"","userId":"09477290535038822979"}},"outputId":"54124d66-a5bb-4958-cf59-f097cfe88955","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%cd /content/drive/My Drive/Case Presentation 1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Case Presentation 1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QR3Da0fcdFxi"},"source":["import torch\n","#!pip install torchtext\n","from torchtext import data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dXgdOaRfdFxl"},"source":["torch.manual_seed(2020);\n","torch.backends.cudnn.deterministic = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z89MAi50dFxp"},"source":["#!pip install spacy\n","# !python -m spacy download en\n","import spacy\n","TEXT = data.Field(tokenize='spacy',batch_first=True,include_lengths=True)\n","LABEL = data.LabelField(dtype = torch.float,batch_first=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B3Kma1zzdFxr"},"source":["fields = [(None, None), ('text',TEXT),('label', LABEL)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2XfZT9oddFxu","executionInfo":{"status":"error","timestamp":1602655446416,"user_tz":-480,"elapsed":2544,"user":{"displayName":"陳柏瑞","photoUrl":"","userId":"16781958207177305804"}},"outputId":"acc4e724-cc88-4289-8cf8-292f98c29685","colab":{"base_uri":"https://localhost:8080/","height":180}},"source":["training_data=data.TabularDataset(path = 'data.csv',format = 'csv',fields = fields,skip_header = True)\n","\n","print(vars(training_data.examples[17]))"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-e229169a4a50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTabularDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskip_header\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"]}]},{"cell_type":"code","metadata":{"id":"9sS5zT3DdFxx"},"source":["import random\n","train_data, valid_data = training_data.split(split_ratio=0.7, random_state = random.seed(2023))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CHs0lUxodFx0","executionInfo":{"status":"ok","timestamp":1602552285131,"user_tz":-480,"elapsed":2883,"user":{"displayName":"張哲魁","photoUrl":"","userId":"09477290535038822979"}},"outputId":"357a1dbd-5a0b-4bf9-99f6-d95c81fc728a","colab":{"base_uri":"https://localhost:8080/","height":104}},"source":["#initialize glove embeddings\n","TEXT.build_vocab(train_data,min_freq=3,vectors = \"glove.6B.100d\")  \n","LABEL.build_vocab(train_data)\n","\n","#No. of unique tokens in text\n","print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n","\n","#No. of unique tokens in label\n","print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n","\n","#Commonly used words\n","print(TEXT.vocab.freqs.most_common(10))  \n","\n","#Word dictionary\n","print(TEXT.vocab.stoi) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Size of TEXT vocabulary: 746\n","Size of LABEL vocabulary: 4\n","[(',', 576), (':', 564), ('and', 235), ('the', 182), ('_', 179), ('*', 178), ('of', 171), ('was', 171), ('-', 135), ('to', 132)]\n","defaultdict(<function _default_unk_index at 0x7f7592e09730>, {'<unk>': 0, '<pad>': 1, ',': 2, ':': 3, 'and': 4, 'the': 5, '_': 6, '*': 7, 'of': 8, 'was': 9, '-': 10, 'to': 11, 'with': 12, 'a': 13, 'in': 14, '/': 15, 'patient': 16, '.': 17, 'for': 18, ')': 19, '(': 20, 'on': 21, '.She': 22, 'is': 23, 'no': 24, '.The': 25, 'at': 26, 'pain': 27, 'PO': 28, 'her': 29, ';': 30, 'HISTORY': 31, 'or': 32, 'she': 33, 'day': 34, 'had': 35, 'left': 36, 'DISCHARGE': 37, 'DIAGNOSIS': 38, 'Status': 39, 'an': 40, 'by': 41, 'has': 42, 'Date': 43, 'NORMAL': 44, 'history': 45, \"'s\": 46, '12:00:00': 47, 'MG': 48, 'mg': 49, 'showed': 50, 'Dr.': 51, '2': 52, 'OF': 53, 'The': 54, 'were': 55, 'DATE': 56, 'M.D.': 57, '.He': 58, 'chest': 59, 'normal': 60, '1': 61, 'PROCEDURES': 62, '.PAST': 63, 'daily': 64, 'right': 65, '.Her': 66, 'p.o': 67, 'well': 68, 'without': 69, 'admission': 70, 'have': 71, 'old': 72, 'COURSE': 73, '[': 74, ']': 75, 'one': 76, 'report_end': 77, 'which': 78, 'Discharge': 79, 'ON': 80, 'post': 81, 'No': 82, 'up': 83, 'use': 84, 'x': 85, 'EXAMINATION': 86, 'as': 87, 'discharged': 88, 'follow': 89, 'his': 90, 'that': 91, '.MEDICATIONS': 92, '10': 93, 'exam': 94, 'postoperative': 95, 'room': 96, 'this': 97, 'will': 98, 'AMDischarge': 99, 'AND': 100, 'D': 101, 'ILLNESS': 102, 'PRESENT': 103, 'any': 104, 'who': 105, '.HISTORY': 106, 'C': 107, 'CT': 108, 'be': 109, 'but': 110, 'negative': 111, 'not': 112, 'two': 113, '.PHYSICAL': 114, '3': 115, 'MEDICAL': 116, 'MEDICATIONS': 117, 'ORDERS': 118, 'abdominal': 119, 'admitted': 120, 'disease': 121, 'home': 122, 'prior': 123, 'regular': 124, 'w': 125, 'year': 126, '.HOSPITAL': 127, 'NUMBER': 128, 'On': 129, 'QD': 130, 'She': 131, 'also': 132, 'cm': 133, 'condition': 134, 'coronary': 135, 'he': 136, 'mass': 137, 'per': 138, 'surgery': 139, '.ALLERGIES': 140, '5': 141, 'Hospital': 142, 'SUMMARYNAME': 143, 'T': 144, 'bypass': 145, 'cardiac': 146, 'clear': 147, 'course': 148, 'drug': 149, 'female': 150, 'hypertension': 151, 'medications': 152, 'past': 153, 'rate': 154, 's': 155, 'shoulder': 156, 'stable': 157, 'taken': 158, 'tobacco': 159, 'week': 160, '.His': 161, '.On': 162, '.SOCIAL': 163, '4': 164, ':1': 165, 'FOR': 166, 'Patient': 167, 'S': 168, 'SummarySignedDISReport': 169, 'are': 170, 'complications': 171, 'depression': 172, 'diet': 173, 'discharge': 174, 'from': 175, 'good': 176, 'pressure': 177, 'scheduled': 178, 'soft': 179, 'some': 180, 'weeks': 181, 'years': 182, '6': 183, '7': 184, 'APPOINTMENT': 185, 'AT': 186, 'Center': 187, 'HOSPITAL': 188, 'Medical': 189, 'PM': 190, 'PROCEDURE': 191, 'Percocet': 192, 'Signed': 193, 'THE': 194, 'UP': 195, 'after': 196, 'bilateral': 197, 'blood': 198, 'breast': 199, 'call': 200, 'delivery': 201, 'did': 202, 'emergency': 203, 'following': 204, 'hospital': 205, 'motion': 206, 'obtained': 207, 'presented': 208, 'revealed': 209, 'there': 210, 'time': 211, 'underwent': 212, '#': 213, '+': 214, '.Patient': 215, '.There': 216, '20': 217, '40': 218, 'AS': 219, 'Admission': 220, 'CP': 221, 'DIAGNOSES': 222, 'IN': 223, 'STATUS': 224, 'SUMMARY': 225, 'SignedDischarge': 226, 'Study': 227, 'THERAPEUTIC': 228, 'artery': 229, 'back': 230, 'biopsy': 231, 'both': 232, 'changes': 233, 'crack': 234, 'diabetes': 235, 'down': 236, 'during': 237, 'found': 238, 'hand': 239, 'it': 240, 'known': 241, 'multiple': 242, 'operating': 243, 'out': 244, 'pelvic': 245, 'performed': 246, 'range': 247, 'scan': 248, 'tender': 249, 'then': 250, 'times': 251, '\"': 252, '.ASSOCIATED': 253, '.Abdomen': 254, '.DISCHARGE': 255, '.FAMILY': 256, '.Heart': 257, '100': 258, '25': 259, '30': 260, '80': 261, '9': 262, 'A': 263, 'After': 264, 'BE': 265, 'BID': 266, 'By': 267, 'Causing': 268, 'Complications': 269, 'Conditions': 270, 'EKG': 271, 'Infections': 272, 'NOT': 273, 'None': 274, 'O.R.': 275, 'ORDER': 276, 'PATIENT': 277, 'POSTPARTUM': 278, 'RESUME': 279, 'Responsible': 280, 'SOB': 281, 'SURGICAL': 282, 'SignedDISCHARGE': 283, 'Stay': 284, 'SummarySignedDISAdmission': 285, 'TREATMENTS': 286, 'Treatment': 287, 'USED': 288, 'YESAttending': 289, 'abuse': 290, 'affecting': 291, 'ago': 292, 'bid': 293, 'bowel': 294, 'complication': 295, 'days': 296, 'decreased': 297, 'degree': 298, 'drinks': 299, 'encountered': 300, 'evaluation': 301, 'felt': 302, 'gastric': 303, 'given': 304, 'heart': 305, 'hematocrit': 306, 'if': 307, 'included': 308, 'likely': 309, 'low': 310, 'lower': 311, 'masses': 312, 'noted': 313, 'p': 314, 'pm': 315, 'po': 316, 'positive': 317, 'ray': 318, 'repair': 319, 'rhythm': 320, 'secondary': 321, 'shortness': 322, 'shows': 323, 'status': 324, 'tolerated': 325, 'uncomplicated': 326, 'uterus': 327, 'vaginal': 328, 'where': 329, '.2': 330, '.However': 331, '.In': 332, '.LABORATORY': 333, '.This': 334, '50': 335, '81': 336, 'ApplicableWILL': 337, 'ECHO': 338, 'ED': 339, 'ETT': 340, 'Electronically': 341, 'Full': 342, 'HTN': 343, 'HomeDISCHARGE': 344, 'I': 345, 'IV': 346, 'L': 347, 'M.D.CODE': 348, 'M.D.DICTATING': 349, 'M.D.TR': 350, 'MINERALS': 351, 'OPERATIONS': 352, 'OR': 353, 'PMCONTINGENT': 354, 'PRN': 355, 'R': 356, 'ST': 357, 'This': 358, 'UPONNot': 359, 'Vaginal': 360, 'X': 361, 'acute': 362, 'alcohol': 363, 'allergies': 364, 'aneurysm': 365, 'aortic': 366, 'bilaterally': 367, 'breath': 368, 'codeDISPOSITION': 369, 'cuff': 370, 'cyst': 371, 'degrees': 372, 'denies': 373, 'edema': 374, 'endometrial': 375, 'fibroid': 376, 'graft': 377, 'increasing': 378, 'intact': 379, 'labor': 380, 'leg': 381, 'lungs': 382, 'male': 383, 'mm': 384, 'node': 385, 'non': 386, 'none': 387, 'nontender': 388, 'o': 389, 'obesity': 390, 'only': 391, 'oral': 392, 'other': 393, 'ovarian': 394, 'p.r.n': 395, 'placed': 396, 'presents': 397, 'rotator': 398, 'section': 399, 'sent': 400, 'sinus': 401, 'slightly': 402, 'stress': 403, 'therapy': 404, 'tolerating': 405, 'ultrasound': 406, 'when': 407, '--': 408, '.A': 409, '.DISPOSITION': 410, '.During': 411, '.HEENT': 412, '.Lungs': 413, '.No': 414, '.PRINCIPAL': 415, '.Pelvic': 416, '150': 417, '2/6/2005': 418, '24': 419, '60': 420, '8': 421, '?': 422, 'ACID': 423, 'ALLERGIES': 424, 'ALLERGY': 425, 'As': 426, 'C5': 427, 'CAD': 428, 'CONDITION': 429, 'CREDKOTE': 430, 'Care': 431, 'Colace': 432, 'DATA': 433, 'DOE': 434, 'HCL': 435, 'HOSPITALIZATION': 436, 'House': 437, 'Hypertension': 438, 'INTERACTION': 439, 'Instructions': 440, 'LISINOPRIL': 441, 'Lipitor': 442, 'MD': 443, 'MI': 444, 'Memorial': 445, 'Norvasc': 446, 'Override': 447, 'PE': 448, 'REASON': 449, 'SERIOUS': 450, 'SummaryUnsignedDISReport': 451, 'TAB': 452, 'added': 453, 'advanced': 454, 'afebrile': 455, 'all': 456, 'anterior': 457, 'antibiotics': 458, 'appendectomy': 459, 'approximately': 460, 'arteries': 461, 'auscultation': 462, 'benign': 463, 'cancer': 464, 'care': 465, 'cervical': 466, 'chronic': 467, 'controlled': 468, 'cord': 469, 'cough': 470, 'duct': 471, 'elevated': 472, 'emesis': 473, 'except': 474, 'extremities': 475, 'extremity': 476, 'fever': 477, 'floor': 478, 'further': 479, 'greater': 480, 'healthy': 481, 'here': 482, 'hours': 483, 'however': 484, 'hyperlipidemia': 485, 'include': 486, 'instructions': 487, 'level': 488, 'ligation': 489, 'mellitus': 490, 'min': 491, 'minutes': 492, 'nausea': 493, 'next': 494, 'night': 495, 'nondistended': 496, 'note': 497, 'now': 498, 'occasional': 499, 'otherwise': 500, 'over': 501, 'override': 502, 'palpable': 503, 'pancreas': 504, 'postoperatively': 505, 'prn': 506, 'pulse': 507, 'reflux': 508, 'remained': 509, 'rule': 510, 'seen': 511, 'significant': 512, 'signs': 513, 'smokes': 514, 'smoking': 515, 'spine': 516, 'surgical': 517, 'tenderness': 518, 'three': 519, 'treatment': 520, 'upper': 521, 'vomiting': 522, 'x3': 523, 'your': 524, '.3': 525, '.ADDITIONAL': 526, '.An': 527, '.EKG': 528, '.HABITS': 529, '.OPERATIONS': 530, '.PCP': 531, '.Postoperatively': 532, '.Pt': 533, '02/11/2003': 534, '04/05/2003': 535, '12:00': 536, '16': 537, '22': 538, '2nd': 539, '45': 540, '52-year': 541, '72': 542, '9.5': 543, '>': 544, 'ADMISSION': 545, 'AI': 546, 'ALEN': 547, 'AMED': 548, 'AMOB': 549, 'ANDFA': 550, 'ASA': 551, 'Abnormal': 552, 'And': 553, 'Aspirin': 554, 'Atenolol': 555, 'Atypical': 556, 'Augmentin': 557, 'BY': 558, 'CARE': 559, 'CHF': 560, 'COMMENTS': 561, 'COPD': 562, 'Carlecant': 563, 'Chest': 564, 'DDISCHARGE': 565, 'DO': 566, 'DeliveryASSOCIATED': 567, 'EGD': 568, 'EMR': 569, 'END': 570, 'Emergency': 571, 'GYN': 572, 'General': 573, 'Gravida': 574, 'HEIGHT': 575, 'Health': 576, 'IM': 577, 'INH': 578, 'IVP': 579, 'KUB': 580, 'LAA': 581, 'LaborPHYSICAL': 582, 'Low': 583, 'MEDDISCHARGE': 584, 'MIEZETRI': 585, 'MRCP': 586, 'MVI': 587, 'May': 588, 'Ms.': 589, 'NIMIRY': 590, 'Name': 591, 'Negative': 592, 'Not': 593, 'Number': 594, 'OPERATION': 595, 'OTHER': 596, 'P': 597, 'PLAN': 598, 'POP': 599, 'PUFF': 600, 'Pain': 601, 'Pap': 602, 'Para': 603, 'RETURN': 604, 'ROAON': 605, 'Resume': 606, 'Rocgelt': 607, 'Room': 608, 'Routine': 609, 'SIMVASTATIN': 610, 'SOUTH': 611, 'STABLE': 612, 'Spontaneous': 613, 'Stable': 614, 'StableTO': 615, 'Starting': 616, 'TCA': 617, 'THIAMINE': 618, 'UnsignedED': 619, 'W': 620, 'Woodmer': 621, 'Wythe': 622, 'Z': 623, 'abdomen': 624, 'able': 625, 'adnexa': 626, 'again': 627, 'age': 628, 'ambulating': 629, 'anesthesia': 630, 'appointment': 631, 'bilirubin': 632, 'bladder': 633, 'catheterization': 634, 'cc': 635, 'cellulitis': 636, 'cesarean': 637, 'change': 638, 'chol': 639, 'cholecystectomy': 640, 'concerns': 641, 'consulted': 642, 'contrast': 643, 'dictated': 644, 'difficulty': 645, 'diseases': 646, 'distress': 647, 'doctor': 648, 'does': 649, 'done': 650, 'dry': 651, 'due': 652, 'enlarged': 653, 'eval': 654, 'evaluated': 655, 'evidence': 656, 'exerciseFOLLOW': 657, 'exercised': 658, 'fashion': 659, 'fevers': 660, 'five': 661, 'flat': 662, 'flexion': 663, 'fluid': 664, 'followed': 665, 'gallops': 666, 'general': 667, 'gram': 668, 'healed': 669, 'held': 670, 'hysterectomy': 671, 'improved': 672, 'increased': 673, 'indication': 674, 'intravenous': 675, 'kept': 676, 'laparoscopic': 677, 'leak': 678, 'lesion': 679, 'limits': 680, 'lives': 681, 'long': 682, 'loss': 683, 'lymph': 684, 'maintained': 685, 'medical': 686, 'months': 687, 'murmur': 688, 'murmurs': 689, 'need': 690, 'number': 691, 'numbness': 692, 'obese': 693, 'office': 694, 'operatively': 695, 'pancreatic': 696, 'polysubstance': 697, 'possible': 698, 'procedure': 699, 'process': 700, 'prolonged': 701, 'pt': 702, 'q': 703, 'qd': 704, 'quit': 705, 'report': 706, 'return': 707, 'risk': 708, 'rubs': 709, 'sat': 710, 'scene': 711, 'schizoaffective': 712, 'several': 713, 'should': 714, 'six': 715, 'spontaneous': 716, 'stay': 717, 'still': 718, 'sugars': 719, 'summaryENTERED': 720, 'symptoms': 721, 'tabs': 722, 'tachycardia': 723, 'test': 724, 'than': 725, 'thoracic': 726, 'throughout': 727, 'tightness': 728, 'total': 729, 'treated': 730, 'tubal': 731, 'tube': 732, 'unremarkable': 733, 'vagina': 734, 'ventricular': 735, 'vital': 736, 'washings': 737, 'wave': 738, 'weakness': 739, 'within': 740, 'woman': 741, 'work': 742, 'wound': 743, 'yo': 744, 'you': 745})\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"muoIAazUdFx3","executionInfo":{"status":"ok","timestamp":1602552285133,"user_tz":-480,"elapsed":2877,"user":{"displayName":"張哲魁","photoUrl":"","userId":"09477290535038822979"}},"outputId":"5cbc146b-1a3f-4540-cdde-ebf54827f7e8","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#check whether cuda is available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n","\n","#set batch size\n","BATCH_SIZE = 7\n","\n","#Load an iterator\n","train_iterator, valid_iterator = data.BucketIterator.splits(\n","    (train_data, valid_data), \n","    batch_size = BATCH_SIZE,\n","    sort_key = lambda x: len(x.text),\n","    sort_within_batch=True,\n","    device = device)\n","device"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"gQfuzOwHdFx5"},"source":["import torch.nn as nn\n","\n","class classifier(nn.Module):\n","    \n","    #define all the layers used in model\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n","                 bidirectional, dropout):\n","        \n","        #Constructor\n","        super().__init__()          \n","        \n","        #embedding layer\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        \n","        #lstm layer\n","        self.lstm = nn.LSTM(embedding_dim, \n","                           hidden_dim, \n","                           num_layers=n_layers, \n","                           bidirectional=bidirectional, \n","                           dropout=dropout,\n","                           batch_first=True)\n","        \n","        #dense layer\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n","\n","        #activation function\n","        self.act = nn.Sigmoid()\n","        \n","    def forward(self, text, text_lengths):\n","        \n","        #text = [batch size,sent_length]\n","        self.embedding = self.embedding\n","        embedded = self.embedding(text)\n","        #embedded = [batch size, sent_len, emb dim]\n","      \n","        #packed sequence\n","        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths,batch_first=True)\n","        \n","        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n","        #hidden = [batch size, num layers * num directions,hid dim]\n","        #cell = [batch size, num layers * num directions,hid dim]\n","        \n","        #concat the final forward and backward hidden state\n","        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n","                \n","        #hidden = [batch size, hid dim * num directions]\n","        dense_outputs=self.fc(hidden)\n","\n","        #Final activation function\n","        outputs=self.act(dense_outputs)\n","        \n","        return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DviyJg6AdFx7"},"source":["#define hyperparameters\n","size_of_vocab = len(TEXT.vocab)\n","embedding_dim = 100\n","num_hidden_nodes = 32\n","num_output_nodes = 4\n","num_layers = 2\n","bidirection = True\n","dropout = 0.2\n","\n","#instantiate the model\n","model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes,num_output_nodes, num_layers, \n","                   bidirectional = True, dropout = dropout)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9PsZhnfydFx9","executionInfo":{"status":"ok","timestamp":1602552285360,"user_tz":-480,"elapsed":3081,"user":{"displayName":"張哲魁","photoUrl":"","userId":"09477290535038822979"}},"outputId":"1a041f1a-2ca0-487b-f04f-afe3b9a2c1ac","colab":{"base_uri":"https://localhost:8080/","height":151}},"source":["#architecture\n","print(model)\n","\n","#No. of trianable parameters\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    \n","print(f'The model has {count_parameters(model):,} trainable parameters')\n","\n","#Initialize the pretrained embedding\n","pretrained_embeddings = TEXT.vocab.vectors\n","model.embedding.weight.data.copy_(pretrained_embeddings)\n","\n","print(pretrained_embeddings.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["classifier(\n","  (embedding): Embedding(746, 100)\n","  (lstm): LSTM(100, 32, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n","  (fc): Linear(in_features=64, out_features=4, bias=True)\n","  (act): Sigmoid()\n",")\n","The model has 134,252 trainable parameters\n","torch.Size([746, 100])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cwzh_aNHdFyA"},"source":["import torch.optim as optim\n","\n","#define optimizer and loss\n","optimizer = optim.Adam(model.parameters())\n","#optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n","#criterion = nn.BCELoss()\n","criterion = nn.CrossEntropyLoss()\n","\n","#define metric\n","def binary_accuracy(preds, y):\n","    #round predictions to the closest integer\n","    #rounded_preds = torch.round(preds)\n","    \n","    #correct = (rounded_preds == y).float()\n","    correct = (preds == y).float() \n","    acc = correct.sum() / len(correct)\n","    return acc\n","    \n","#push to cuda if available\n","model = model.to(device)\n","criterion = criterion.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qln0W00adFyE"},"source":["def train(model, iterator, optimizer, criterion):\n","    \n","    #initialize every epoch \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    #set the model in training phase\n","    model.train()  \n","    \n","    for batch in iterator:\n","        \n","        #resets the gradients after every batch\n","        optimizer.zero_grad()   \n","        \n","        #retrieve text and no. of words\n","        text, text_lengths = batch.text   \n","        text_lengths = text_lengths.cuda()\n","            \n","        #convert to 1D tensor\n","        #predictions = model(text, text_lengths).squeeze()\n","        \n","        predictions = model(text, text_lengths).squeeze() \n","\n","        #predictions = model(text, text_lengths)\n","        #predictions = torch.max(predictions, 1)[1]\n","        #predictions = predictions.type(torch.FloatTensor)\n","        #predictions = predictions.cuda()\n","        #predictions = predictions.requires_grad_()\n","\n","        #compute the loss\n","        loss = criterion(predictions, batch.label.type(torch.LongTensor).cuda())\n","        \n","        #compute the binary accuracy\n","        acc = binary_accuracy(torch.max(predictions, 1)[1].type(torch.FloatTensor), batch.label.cpu())   \n","        \n","        #backpropage the loss and compute the gradients\n","        loss.backward()       \n","        \n","        #update the weights\n","        optimizer.step()      \n","        \n","        #loss and accuracy\n","        epoch_loss += loss.item()  \n","        epoch_acc += acc.item()    \n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0WKYAb09dFyI"},"source":["def evaluate(model, iterator, criterion):\n","    \n","    #initialize every epoch\n","    epoch_loss = 0\n","    epoch_acc = 0\n","\n","    #deactivating dropout layers\n","    model.eval()\n","    \n","    #deactivates autograd\n","    with torch.no_grad():\n","    \n","        for batch in iterator:\n","        \n","            #retrieve text and no. of words\n","            text, text_lengths = batch.text\n","            text = text.cuda()\n","            text_lengths = text_lengths.cuda()\n","\n","            #convert to 1d tensor\n","            #predictions = model(text, text_lengths).squeeze()\n","\n","            predictions = model(text, text_lengths).squeeze()  \n","\n","            #predictions = model(text, text_lengths)\n","            #predictions = torch.max(predictions, 1)[1]\n","            #predictions = predictions.type(torch.FloatTensor) \n","            #predictions = predictions.cuda()\n","            #predictions = predictions.requires_grad_()\n","\n","            #compute loss and accuracy\n","            loss = criterion(predictions, batch.label.type(torch.LongTensor).cuda())\n","            acc = binary_accuracy(torch.max(predictions, 1)[1].type(torch.FloatTensor), batch.label.cpu())\n","            \n","            #keep track of loss and accuracy\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MLzNDpKLdFyK","executionInfo":{"status":"error","timestamp":1602552404380,"user_tz":-480,"elapsed":122062,"user":{"displayName":"張哲魁","photoUrl":"","userId":"09477290535038822979"}},"outputId":"c7a4288b-e42b-48a6-a9ed-e2fff8f0b8fe","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["N_EPOCHS = 500\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","     \n","    #train the model\n","    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n","    \n","    #evaluate the model\n","    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n","    \n","    #save the best model\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'saved_weights.pt')\n","    print(str(epoch) + f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0\tTrain Loss: 1.388 | Train Acc: 21.43%\n","\t Val. Loss: 1.383 |  Val. Acc: 17.14%\n","1\tTrain Loss: 1.386 | Train Acc: 28.57%\n","\t Val. Loss: 1.383 |  Val. Acc: 17.14%\n","2\tTrain Loss: 1.384 | Train Acc: 28.57%\n","\t Val. Loss: 1.385 |  Val. Acc: 17.14%\n","3\tTrain Loss: 1.381 | Train Acc: 28.57%\n","\t Val. Loss: 1.385 |  Val. Acc: 17.14%\n","4\tTrain Loss: 1.379 | Train Acc: 28.57%\n","\t Val. Loss: 1.386 |  Val. Acc: 17.14%\n","5\tTrain Loss: 1.375 | Train Acc: 28.57%\n","\t Val. Loss: 1.387 |  Val. Acc: 17.14%\n","6\tTrain Loss: 1.371 | Train Acc: 28.57%\n","\t Val. Loss: 1.387 |  Val. Acc: 17.14%\n","7\tTrain Loss: 1.365 | Train Acc: 28.57%\n","\t Val. Loss: 1.386 |  Val. Acc: 17.14%\n","8\tTrain Loss: 1.358 | Train Acc: 28.57%\n","\t Val. Loss: 1.386 |  Val. Acc: 17.14%\n","9\tTrain Loss: 1.351 | Train Acc: 28.57%\n","\t Val. Loss: 1.387 |  Val. Acc: 17.14%\n","10\tTrain Loss: 1.345 | Train Acc: 25.00%\n","\t Val. Loss: 1.386 |  Val. Acc: 17.14%\n","11\tTrain Loss: 1.331 | Train Acc: 32.14%\n","\t Val. Loss: 1.379 |  Val. Acc: 24.29%\n","12\tTrain Loss: 1.320 | Train Acc: 35.71%\n","\t Val. Loss: 1.377 |  Val. Acc: 31.43%\n","13\tTrain Loss: 1.304 | Train Acc: 35.71%\n","\t Val. Loss: 1.370 |  Val. Acc: 31.43%\n","14\tTrain Loss: 1.288 | Train Acc: 39.29%\n","\t Val. Loss: 1.355 |  Val. Acc: 31.43%\n","15\tTrain Loss: 1.265 | Train Acc: 42.86%\n","\t Val. Loss: 1.328 |  Val. Acc: 38.57%\n","16\tTrain Loss: 1.247 | Train Acc: 50.00%\n","\t Val. Loss: 1.320 |  Val. Acc: 38.57%\n","17\tTrain Loss: 1.229 | Train Acc: 53.57%\n","\t Val. Loss: 1.326 |  Val. Acc: 38.57%\n","18\tTrain Loss: 1.203 | Train Acc: 60.71%\n","\t Val. Loss: 1.350 |  Val. Acc: 38.57%\n","19\tTrain Loss: 1.176 | Train Acc: 64.29%\n","\t Val. Loss: 1.314 |  Val. Acc: 48.57%\n","20\tTrain Loss: 1.159 | Train Acc: 78.57%\n","\t Val. Loss: 1.288 |  Val. Acc: 38.57%\n","21\tTrain Loss: 1.142 | Train Acc: 78.57%\n","\t Val. Loss: 1.314 |  Val. Acc: 48.57%\n","22\tTrain Loss: 1.119 | Train Acc: 75.00%\n","\t Val. Loss: 1.363 |  Val. Acc: 41.43%\n","23\tTrain Loss: 1.110 | Train Acc: 75.00%\n","\t Val. Loss: 1.352 |  Val. Acc: 48.57%\n","24\tTrain Loss: 1.085 | Train Acc: 75.00%\n","\t Val. Loss: 1.314 |  Val. Acc: 48.57%\n","25\tTrain Loss: 1.071 | Train Acc: 78.57%\n","\t Val. Loss: 1.295 |  Val. Acc: 48.57%\n","26\tTrain Loss: 1.071 | Train Acc: 75.00%\n","\t Val. Loss: 1.281 |  Val. Acc: 48.57%\n","27\tTrain Loss: 1.058 | Train Acc: 78.57%\n","\t Val. Loss: 1.313 |  Val. Acc: 38.57%\n","28\tTrain Loss: 1.042 | Train Acc: 78.57%\n","\t Val. Loss: 1.361 |  Val. Acc: 48.57%\n","29\tTrain Loss: 1.028 | Train Acc: 82.14%\n","\t Val. Loss: 1.395 |  Val. Acc: 41.43%\n","30\tTrain Loss: 1.035 | Train Acc: 78.57%\n","\t Val. Loss: 1.294 |  Val. Acc: 48.57%\n","31\tTrain Loss: 1.009 | Train Acc: 82.14%\n","\t Val. Loss: 1.341 |  Val. Acc: 38.57%\n","32\tTrain Loss: 1.018 | Train Acc: 78.57%\n","\t Val. Loss: 1.392 |  Val. Acc: 41.43%\n","33\tTrain Loss: 1.005 | Train Acc: 78.57%\n","\t Val. Loss: 1.424 |  Val. Acc: 48.57%\n","34\tTrain Loss: 0.987 | Train Acc: 78.57%\n","\t Val. Loss: 1.387 |  Val. Acc: 48.57%\n","35\tTrain Loss: 0.973 | Train Acc: 85.71%\n","\t Val. Loss: 1.331 |  Val. Acc: 48.57%\n","36\tTrain Loss: 0.978 | Train Acc: 85.71%\n","\t Val. Loss: 1.335 |  Val. Acc: 48.57%\n","37\tTrain Loss: 0.973 | Train Acc: 89.29%\n","\t Val. Loss: 1.331 |  Val. Acc: 41.43%\n","38\tTrain Loss: 0.951 | Train Acc: 92.86%\n","\t Val. Loss: 1.355 |  Val. Acc: 48.57%\n","39\tTrain Loss: 0.948 | Train Acc: 92.86%\n","\t Val. Loss: 1.329 |  Val. Acc: 48.57%\n","40\tTrain Loss: 0.945 | Train Acc: 85.71%\n","\t Val. Loss: 1.252 |  Val. Acc: 38.57%\n","41\tTrain Loss: 0.950 | Train Acc: 85.71%\n","\t Val. Loss: 1.254 |  Val. Acc: 38.57%\n","42\tTrain Loss: 0.933 | Train Acc: 92.86%\n","\t Val. Loss: 1.308 |  Val. Acc: 48.57%\n","43\tTrain Loss: 0.929 | Train Acc: 92.86%\n","\t Val. Loss: 1.280 |  Val. Acc: 48.57%\n","44\tTrain Loss: 0.949 | Train Acc: 85.71%\n","\t Val. Loss: 1.181 |  Val. Acc: 48.57%\n","45\tTrain Loss: 0.948 | Train Acc: 89.29%\n","\t Val. Loss: 1.245 |  Val. Acc: 31.43%\n","46\tTrain Loss: 0.935 | Train Acc: 85.71%\n","\t Val. Loss: 1.325 |  Val. Acc: 38.57%\n","47\tTrain Loss: 0.927 | Train Acc: 89.29%\n","\t Val. Loss: 1.351 |  Val. Acc: 48.57%\n","48\tTrain Loss: 0.921 | Train Acc: 89.29%\n","\t Val. Loss: 1.270 |  Val. Acc: 38.57%\n","49\tTrain Loss: 0.908 | Train Acc: 89.29%\n","\t Val. Loss: 1.270 |  Val. Acc: 38.57%\n","50\tTrain Loss: 0.908 | Train Acc: 92.86%\n","\t Val. Loss: 1.287 |  Val. Acc: 38.57%\n","51\tTrain Loss: 0.898 | Train Acc: 92.86%\n","\t Val. Loss: 1.318 |  Val. Acc: 38.57%\n","52\tTrain Loss: 0.888 | Train Acc: 92.86%\n","\t Val. Loss: 1.326 |  Val. Acc: 38.57%\n","53\tTrain Loss: 0.878 | Train Acc: 92.86%\n","\t Val. Loss: 1.328 |  Val. Acc: 38.57%\n","54\tTrain Loss: 0.872 | Train Acc: 92.86%\n","\t Val. Loss: 1.337 |  Val. Acc: 28.57%\n","55\tTrain Loss: 0.865 | Train Acc: 92.86%\n","\t Val. Loss: 1.331 |  Val. Acc: 28.57%\n","56\tTrain Loss: 0.860 | Train Acc: 92.86%\n","\t Val. Loss: 1.309 |  Val. Acc: 38.57%\n","57\tTrain Loss: 0.853 | Train Acc: 92.86%\n","\t Val. Loss: 1.295 |  Val. Acc: 38.57%\n","58\tTrain Loss: 0.851 | Train Acc: 92.86%\n","\t Val. Loss: 1.302 |  Val. Acc: 38.57%\n","59\tTrain Loss: 0.843 | Train Acc: 92.86%\n","\t Val. Loss: 1.291 |  Val. Acc: 38.57%\n","60\tTrain Loss: 0.839 | Train Acc: 92.86%\n","\t Val. Loss: 1.087 |  Val. Acc: 65.71%\n","61\tTrain Loss: 0.916 | Train Acc: 85.71%\n","\t Val. Loss: 1.189 |  Val. Acc: 58.57%\n","62\tTrain Loss: 0.886 | Train Acc: 89.29%\n","\t Val. Loss: 1.268 |  Val. Acc: 41.43%\n","63\tTrain Loss: 0.873 | Train Acc: 85.71%\n","\t Val. Loss: 1.264 |  Val. Acc: 38.57%\n","64\tTrain Loss: 0.843 | Train Acc: 92.86%\n","\t Val. Loss: 1.204 |  Val. Acc: 58.57%\n","65\tTrain Loss: 0.829 | Train Acc: 92.86%\n","\t Val. Loss: 1.188 |  Val. Acc: 58.57%\n","66\tTrain Loss: 0.835 | Train Acc: 92.86%\n","\t Val. Loss: 1.210 |  Val. Acc: 58.57%\n","67\tTrain Loss: 0.833 | Train Acc: 92.86%\n","\t Val. Loss: 1.228 |  Val. Acc: 58.57%\n","68\tTrain Loss: 0.823 | Train Acc: 92.86%\n","\t Val. Loss: 1.213 |  Val. Acc: 58.57%\n","69\tTrain Loss: 0.818 | Train Acc: 92.86%\n","\t Val. Loss: 1.206 |  Val. Acc: 48.57%\n","70\tTrain Loss: 0.815 | Train Acc: 92.86%\n","\t Val. Loss: 1.213 |  Val. Acc: 48.57%\n","71\tTrain Loss: 0.811 | Train Acc: 92.86%\n","\t Val. Loss: 1.232 |  Val. Acc: 48.57%\n","72\tTrain Loss: 0.809 | Train Acc: 92.86%\n","\t Val. Loss: 1.244 |  Val. Acc: 48.57%\n","73\tTrain Loss: 0.808 | Train Acc: 92.86%\n","\t Val. Loss: 1.245 |  Val. Acc: 48.57%\n","74\tTrain Loss: 0.804 | Train Acc: 92.86%\n","\t Val. Loss: 1.239 |  Val. Acc: 48.57%\n","75\tTrain Loss: 0.803 | Train Acc: 92.86%\n","\t Val. Loss: 1.238 |  Val. Acc: 48.57%\n","76\tTrain Loss: 0.802 | Train Acc: 92.86%\n","\t Val. Loss: 1.240 |  Val. Acc: 48.57%\n","77\tTrain Loss: 0.799 | Train Acc: 92.86%\n","\t Val. Loss: 1.238 |  Val. Acc: 48.57%\n","78\tTrain Loss: 0.797 | Train Acc: 92.86%\n","\t Val. Loss: 1.243 |  Val. Acc: 48.57%\n","79\tTrain Loss: 0.797 | Train Acc: 92.86%\n","\t Val. Loss: 1.252 |  Val. Acc: 48.57%\n","80\tTrain Loss: 0.795 | Train Acc: 92.86%\n","\t Val. Loss: 1.257 |  Val. Acc: 48.57%\n","81\tTrain Loss: 0.794 | Train Acc: 92.86%\n","\t Val. Loss: 1.264 |  Val. Acc: 48.57%\n","82\tTrain Loss: 0.794 | Train Acc: 92.86%\n","\t Val. Loss: 1.259 |  Val. Acc: 48.57%\n","83\tTrain Loss: 0.792 | Train Acc: 92.86%\n","\t Val. Loss: 1.255 |  Val. Acc: 48.57%\n","84\tTrain Loss: 0.791 | Train Acc: 92.86%\n","\t Val. Loss: 1.257 |  Val. Acc: 48.57%\n","85\tTrain Loss: 0.791 | Train Acc: 92.86%\n","\t Val. Loss: 1.263 |  Val. Acc: 48.57%\n","86\tTrain Loss: 0.790 | Train Acc: 92.86%\n","\t Val. Loss: 1.263 |  Val. Acc: 48.57%\n","87\tTrain Loss: 0.789 | Train Acc: 92.86%\n","\t Val. Loss: 1.270 |  Val. Acc: 38.57%\n","88\tTrain Loss: 0.788 | Train Acc: 92.86%\n","\t Val. Loss: 1.283 |  Val. Acc: 38.57%\n","89\tTrain Loss: 0.787 | Train Acc: 92.86%\n","\t Val. Loss: 1.293 |  Val. Acc: 38.57%\n","90\tTrain Loss: 0.787 | Train Acc: 92.86%\n","\t Val. Loss: 1.297 |  Val. Acc: 38.57%\n","91\tTrain Loss: 0.786 | Train Acc: 92.86%\n","\t Val. Loss: 1.294 |  Val. Acc: 38.57%\n","92\tTrain Loss: 0.785 | Train Acc: 92.86%\n","\t Val. Loss: 1.292 |  Val. Acc: 38.57%\n","93\tTrain Loss: 0.785 | Train Acc: 92.86%\n","\t Val. Loss: 1.292 |  Val. Acc: 38.57%\n","94\tTrain Loss: 0.785 | Train Acc: 92.86%\n","\t Val. Loss: 1.296 |  Val. Acc: 38.57%\n","95\tTrain Loss: 0.784 | Train Acc: 92.86%\n","\t Val. Loss: 1.298 |  Val. Acc: 38.57%\n","96\tTrain Loss: 0.784 | Train Acc: 92.86%\n","\t Val. Loss: 1.297 |  Val. Acc: 38.57%\n","97\tTrain Loss: 0.791 | Train Acc: 92.86%\n","\t Val. Loss: 1.292 |  Val. Acc: 38.57%\n","98\tTrain Loss: 0.783 | Train Acc: 92.86%\n","\t Val. Loss: 1.292 |  Val. Acc: 38.57%\n","99\tTrain Loss: 0.782 | Train Acc: 92.86%\n","\t Val. Loss: 1.296 |  Val. Acc: 38.57%\n","100\tTrain Loss: 0.782 | Train Acc: 92.86%\n","\t Val. Loss: 1.303 |  Val. Acc: 38.57%\n","101\tTrain Loss: 0.781 | Train Acc: 92.86%\n","\t Val. Loss: 1.311 |  Val. Acc: 38.57%\n","102\tTrain Loss: 0.781 | Train Acc: 92.86%\n","\t Val. Loss: 1.331 |  Val. Acc: 28.57%\n","103\tTrain Loss: 0.780 | Train Acc: 92.86%\n","\t Val. Loss: 1.342 |  Val. Acc: 28.57%\n","104\tTrain Loss: 0.780 | Train Acc: 92.86%\n","\t Val. Loss: 1.343 |  Val. Acc: 28.57%\n","105\tTrain Loss: 0.779 | Train Acc: 92.86%\n","\t Val. Loss: 1.348 |  Val. Acc: 28.57%\n","106\tTrain Loss: 0.779 | Train Acc: 92.86%\n","\t Val. Loss: 1.349 |  Val. Acc: 28.57%\n","107\tTrain Loss: 0.778 | Train Acc: 92.86%\n","\t Val. Loss: 1.348 |  Val. Acc: 38.57%\n","108\tTrain Loss: 0.778 | Train Acc: 92.86%\n","\t Val. Loss: 1.343 |  Val. Acc: 38.57%\n","109\tTrain Loss: 0.778 | Train Acc: 92.86%\n","\t Val. Loss: 1.338 |  Val. Acc: 38.57%\n","110\tTrain Loss: 0.778 | Train Acc: 92.86%\n","\t Val. Loss: 1.336 |  Val. Acc: 38.57%\n","111\tTrain Loss: 0.777 | Train Acc: 92.86%\n","\t Val. Loss: 1.338 |  Val. Acc: 38.57%\n","112\tTrain Loss: 0.777 | Train Acc: 92.86%\n","\t Val. Loss: 1.330 |  Val. Acc: 38.57%\n","113\tTrain Loss: 0.776 | Train Acc: 92.86%\n","\t Val. Loss: 1.313 |  Val. Acc: 38.57%\n","114\tTrain Loss: 0.776 | Train Acc: 92.86%\n","\t Val. Loss: 1.313 |  Val. Acc: 38.57%\n","115\tTrain Loss: 0.776 | Train Acc: 92.86%\n","\t Val. Loss: 1.318 |  Val. Acc: 38.57%\n","116\tTrain Loss: 0.776 | Train Acc: 92.86%\n","\t Val. Loss: 1.322 |  Val. Acc: 38.57%\n","117\tTrain Loss: 0.775 | Train Acc: 92.86%\n","\t Val. Loss: 1.328 |  Val. Acc: 38.57%\n","118\tTrain Loss: 0.775 | Train Acc: 92.86%\n","\t Val. Loss: 1.328 |  Val. Acc: 38.57%\n","119\tTrain Loss: 0.775 | Train Acc: 92.86%\n","\t Val. Loss: 1.329 |  Val. Acc: 28.57%\n","120\tTrain Loss: 0.775 | Train Acc: 92.86%\n","\t Val. Loss: 1.331 |  Val. Acc: 28.57%\n","121\tTrain Loss: 0.774 | Train Acc: 92.86%\n","\t Val. Loss: 1.336 |  Val. Acc: 28.57%\n","122\tTrain Loss: 0.774 | Train Acc: 92.86%\n","\t Val. Loss: 1.338 |  Val. Acc: 28.57%\n","123\tTrain Loss: 0.774 | Train Acc: 92.86%\n","\t Val. Loss: 1.339 |  Val. Acc: 28.57%\n","124\tTrain Loss: 0.774 | Train Acc: 96.43%\n","\t Val. Loss: 1.343 |  Val. Acc: 28.57%\n","125\tTrain Loss: 0.773 | Train Acc: 92.86%\n","\t Val. Loss: 1.349 |  Val. Acc: 28.57%\n","126\tTrain Loss: 0.773 | Train Acc: 92.86%\n","\t Val. Loss: 1.350 |  Val. Acc: 28.57%\n","127\tTrain Loss: 0.773 | Train Acc: 96.43%\n","\t Val. Loss: 1.347 |  Val. Acc: 28.57%\n","128\tTrain Loss: 0.773 | Train Acc: 92.86%\n","\t Val. Loss: 1.345 |  Val. Acc: 28.57%\n","129\tTrain Loss: 0.773 | Train Acc: 96.43%\n","\t Val. Loss: 1.342 |  Val. Acc: 28.57%\n","130\tTrain Loss: 0.773 | Train Acc: 96.43%\n","\t Val. Loss: 1.341 |  Val. Acc: 28.57%\n","131\tTrain Loss: 0.772 | Train Acc: 96.43%\n","\t Val. Loss: 1.335 |  Val. Acc: 38.57%\n","132\tTrain Loss: 0.772 | Train Acc: 100.00%\n","\t Val. Loss: 1.338 |  Val. Acc: 38.57%\n","133\tTrain Loss: 0.772 | Train Acc: 100.00%\n","\t Val. Loss: 1.341 |  Val. Acc: 28.57%\n","134\tTrain Loss: 0.772 | Train Acc: 100.00%\n","\t Val. Loss: 1.342 |  Val. Acc: 28.57%\n","135\tTrain Loss: 0.772 | Train Acc: 100.00%\n","\t Val. Loss: 1.346 |  Val. Acc: 28.57%\n","136\tTrain Loss: 0.771 | Train Acc: 100.00%\n","\t Val. Loss: 1.348 |  Val. Acc: 28.57%\n","137\tTrain Loss: 0.771 | Train Acc: 100.00%\n","\t Val. Loss: 1.347 |  Val. Acc: 28.57%\n","138\tTrain Loss: 0.771 | Train Acc: 100.00%\n","\t Val. Loss: 1.348 |  Val. Acc: 28.57%\n","139\tTrain Loss: 0.771 | Train Acc: 100.00%\n","\t Val. Loss: 1.349 |  Val. Acc: 28.57%\n","140\tTrain Loss: 0.771 | Train Acc: 100.00%\n","\t Val. Loss: 1.335 |  Val. Acc: 28.57%\n","141\tTrain Loss: 0.770 | Train Acc: 100.00%\n","\t Val. Loss: 1.325 |  Val. Acc: 28.57%\n","142\tTrain Loss: 0.768 | Train Acc: 100.00%\n","\t Val. Loss: 1.307 |  Val. Acc: 38.57%\n","143\tTrain Loss: 0.781 | Train Acc: 100.00%\n","\t Val. Loss: 1.366 |  Val. Acc: 28.57%\n","144\tTrain Loss: 0.797 | Train Acc: 96.43%\n","\t Val. Loss: 1.332 |  Val. Acc: 38.57%\n","145\tTrain Loss: 0.840 | Train Acc: 89.29%\n","\t Val. Loss: 1.252 |  Val. Acc: 38.57%\n","146\tTrain Loss: 0.862 | Train Acc: 89.29%\n","\t Val. Loss: 1.350 |  Val. Acc: 31.43%\n","147\tTrain Loss: 0.940 | Train Acc: 82.14%\n","\t Val. Loss: 1.226 |  Val. Acc: 51.43%\n","148\tTrain Loss: 0.877 | Train Acc: 89.29%\n","\t Val. Loss: 1.337 |  Val. Acc: 41.43%\n","149\tTrain Loss: 0.829 | Train Acc: 89.29%\n","\t Val. Loss: 1.370 |  Val. Acc: 38.57%\n","150\tTrain Loss: 0.826 | Train Acc: 96.43%\n","\t Val. Loss: 1.351 |  Val. Acc: 38.57%\n","151\tTrain Loss: 0.796 | Train Acc: 96.43%\n","\t Val. Loss: 1.358 |  Val. Acc: 38.57%\n","152\tTrain Loss: 0.790 | Train Acc: 92.86%\n","\t Val. Loss: 1.353 |  Val. Acc: 38.57%\n","153\tTrain Loss: 0.779 | Train Acc: 92.86%\n","\t Val. Loss: 1.344 |  Val. Acc: 38.57%\n","154\tTrain Loss: 0.777 | Train Acc: 96.43%\n","\t Val. Loss: 1.326 |  Val. Acc: 38.57%\n","155\tTrain Loss: 0.776 | Train Acc: 100.00%\n","\t Val. Loss: 1.301 |  Val. Acc: 38.57%\n","156\tTrain Loss: 0.773 | Train Acc: 100.00%\n","\t Val. Loss: 1.281 |  Val. Acc: 48.57%\n","157\tTrain Loss: 0.771 | Train Acc: 100.00%\n","\t Val. Loss: 1.274 |  Val. Acc: 48.57%\n","158\tTrain Loss: 0.772 | Train Acc: 100.00%\n","\t Val. Loss: 1.264 |  Val. Acc: 48.57%\n","159\tTrain Loss: 0.770 | Train Acc: 100.00%\n","\t Val. Loss: 1.263 |  Val. Acc: 48.57%\n","160\tTrain Loss: 0.768 | Train Acc: 100.00%\n","\t Val. Loss: 1.265 |  Val. Acc: 48.57%\n","161\tTrain Loss: 0.767 | Train Acc: 100.00%\n","\t Val. Loss: 1.281 |  Val. Acc: 48.57%\n","162\tTrain Loss: 0.763 | Train Acc: 100.00%\n","\t Val. Loss: 1.289 |  Val. Acc: 48.57%\n","163\tTrain Loss: 0.762 | Train Acc: 100.00%\n","\t Val. Loss: 1.290 |  Val. Acc: 48.57%\n","164\tTrain Loss: 0.761 | Train Acc: 100.00%\n","\t Val. Loss: 1.291 |  Val. Acc: 48.57%\n","165\tTrain Loss: 0.760 | Train Acc: 100.00%\n","\t Val. Loss: 1.294 |  Val. Acc: 48.57%\n","166\tTrain Loss: 0.760 | Train Acc: 100.00%\n","\t Val. Loss: 1.296 |  Val. Acc: 48.57%\n","167\tTrain Loss: 0.759 | Train Acc: 100.00%\n","\t Val. Loss: 1.300 |  Val. Acc: 48.57%\n","168\tTrain Loss: 0.759 | Train Acc: 100.00%\n","\t Val. Loss: 1.310 |  Val. Acc: 38.57%\n","169\tTrain Loss: 0.759 | Train Acc: 100.00%\n","\t Val. Loss: 1.320 |  Val. Acc: 38.57%\n","170\tTrain Loss: 0.758 | Train Acc: 100.00%\n","\t Val. Loss: 1.326 |  Val. Acc: 38.57%\n","171\tTrain Loss: 0.758 | Train Acc: 100.00%\n","\t Val. Loss: 1.329 |  Val. Acc: 38.57%\n","172\tTrain Loss: 0.758 | Train Acc: 100.00%\n","\t Val. Loss: 1.330 |  Val. Acc: 38.57%\n","173\tTrain Loss: 0.757 | Train Acc: 100.00%\n","\t Val. Loss: 1.334 |  Val. Acc: 38.57%\n","174\tTrain Loss: 0.756 | Train Acc: 100.00%\n","\t Val. Loss: 1.336 |  Val. Acc: 38.57%\n","175\tTrain Loss: 0.756 | Train Acc: 100.00%\n","\t Val. Loss: 1.338 |  Val. Acc: 38.57%\n","176\tTrain Loss: 0.756 | Train Acc: 100.00%\n","\t Val. Loss: 1.339 |  Val. Acc: 38.57%\n","177\tTrain Loss: 0.755 | Train Acc: 100.00%\n","\t Val. Loss: 1.341 |  Val. Acc: 38.57%\n","178\tTrain Loss: 0.755 | Train Acc: 100.00%\n","\t Val. Loss: 1.344 |  Val. Acc: 38.57%\n","179\tTrain Loss: 0.755 | Train Acc: 100.00%\n","\t Val. Loss: 1.346 |  Val. Acc: 38.57%\n","180\tTrain Loss: 0.754 | Train Acc: 100.00%\n","\t Val. Loss: 1.348 |  Val. Acc: 38.57%\n","181\tTrain Loss: 0.754 | Train Acc: 100.00%\n","\t Val. Loss: 1.351 |  Val. Acc: 38.57%\n","182\tTrain Loss: 0.754 | Train Acc: 100.00%\n","\t Val. Loss: 1.353 |  Val. Acc: 38.57%\n","183\tTrain Loss: 0.753 | Train Acc: 100.00%\n","\t Val. Loss: 1.354 |  Val. Acc: 38.57%\n","184\tTrain Loss: 0.753 | Train Acc: 100.00%\n","\t Val. Loss: 1.353 |  Val. Acc: 38.57%\n","185\tTrain Loss: 0.753 | Train Acc: 100.00%\n","\t Val. Loss: 1.353 |  Val. Acc: 38.57%\n","186\tTrain Loss: 0.752 | Train Acc: 100.00%\n","\t Val. Loss: 1.352 |  Val. Acc: 38.57%\n","187\tTrain Loss: 0.752 | Train Acc: 100.00%\n","\t Val. Loss: 1.353 |  Val. Acc: 38.57%\n","188\tTrain Loss: 0.751 | Train Acc: 100.00%\n","\t Val. Loss: 1.353 |  Val. Acc: 38.57%\n","189\tTrain Loss: 0.751 | Train Acc: 100.00%\n","\t Val. Loss: 1.354 |  Val. Acc: 38.57%\n","190\tTrain Loss: 0.751 | Train Acc: 100.00%\n","\t Val. Loss: 1.355 |  Val. Acc: 38.57%\n","191\tTrain Loss: 0.751 | Train Acc: 100.00%\n","\t Val. Loss: 1.357 |  Val. Acc: 38.57%\n","192\tTrain Loss: 0.751 | Train Acc: 100.00%\n","\t Val. Loss: 1.358 |  Val. Acc: 38.57%\n","193\tTrain Loss: 0.751 | Train Acc: 100.00%\n","\t Val. Loss: 1.360 |  Val. Acc: 38.57%\n","194\tTrain Loss: 0.751 | Train Acc: 100.00%\n","\t Val. Loss: 1.362 |  Val. Acc: 38.57%\n","195\tTrain Loss: 0.751 | Train Acc: 100.00%\n","\t Val. Loss: 1.365 |  Val. Acc: 38.57%\n","196\tTrain Loss: 0.750 | Train Acc: 100.00%\n","\t Val. Loss: 1.369 |  Val. Acc: 38.57%\n","197\tTrain Loss: 0.750 | Train Acc: 100.00%\n","\t Val. Loss: 1.372 |  Val. Acc: 38.57%\n","198\tTrain Loss: 0.750 | Train Acc: 100.00%\n","\t Val. Loss: 1.374 |  Val. Acc: 38.57%\n","199\tTrain Loss: 0.750 | Train Acc: 100.00%\n","\t Val. Loss: 1.379 |  Val. Acc: 38.57%\n","200\tTrain Loss: 0.750 | Train Acc: 100.00%\n","\t Val. Loss: 1.380 |  Val. Acc: 38.57%\n","201\tTrain Loss: 0.750 | Train Acc: 100.00%\n","\t Val. Loss: 1.386 |  Val. Acc: 28.57%\n","202\tTrain Loss: 0.750 | Train Acc: 100.00%\n","\t Val. Loss: 1.391 |  Val. Acc: 28.57%\n","203\tTrain Loss: 0.750 | Train Acc: 100.00%\n","\t Val. Loss: 1.397 |  Val. Acc: 28.57%\n","204\tTrain Loss: 0.750 | Train Acc: 100.00%\n","\t Val. Loss: 1.403 |  Val. Acc: 28.57%\n","205\tTrain Loss: 0.750 | Train Acc: 100.00%\n","\t Val. Loss: 1.409 |  Val. Acc: 28.57%\n","206\tTrain Loss: 0.750 | Train Acc: 100.00%\n","\t Val. Loss: 1.415 |  Val. Acc: 28.57%\n","207\tTrain Loss: 0.749 | Train Acc: 100.00%\n","\t Val. Loss: 1.419 |  Val. Acc: 28.57%\n","208\tTrain Loss: 0.749 | Train Acc: 100.00%\n","\t Val. Loss: 1.428 |  Val. Acc: 28.57%\n","209\tTrain Loss: 0.749 | Train Acc: 100.00%\n","\t Val. Loss: 1.429 |  Val. Acc: 28.57%\n","210\tTrain Loss: 0.749 | Train Acc: 100.00%\n","\t Val. Loss: 1.431 |  Val. Acc: 28.57%\n","211\tTrain Loss: 0.749 | Train Acc: 100.00%\n","\t Val. Loss: 1.432 |  Val. Acc: 28.57%\n","212\tTrain Loss: 0.749 | Train Acc: 100.00%\n","\t Val. Loss: 1.433 |  Val. Acc: 28.57%\n","213\tTrain Loss: 0.749 | Train Acc: 100.00%\n","\t Val. Loss: 1.434 |  Val. Acc: 28.57%\n","214\tTrain Loss: 0.749 | Train Acc: 100.00%\n","\t Val. Loss: 1.433 |  Val. Acc: 28.57%\n","215\tTrain Loss: 0.749 | Train Acc: 100.00%\n","\t Val. Loss: 1.435 |  Val. Acc: 28.57%\n","216\tTrain Loss: 0.749 | Train Acc: 100.00%\n","\t Val. Loss: 1.436 |  Val. Acc: 28.57%\n","217\tTrain Loss: 0.749 | Train Acc: 100.00%\n","\t Val. Loss: 1.437 |  Val. Acc: 28.57%\n","218\tTrain Loss: 0.749 | Train Acc: 100.00%\n","\t Val. Loss: 1.439 |  Val. Acc: 28.57%\n","219\tTrain Loss: 0.749 | Train Acc: 100.00%\n","\t Val. Loss: 1.440 |  Val. Acc: 28.57%\n","220\tTrain Loss: 0.749 | Train Acc: 100.00%\n","\t Val. Loss: 1.442 |  Val. Acc: 28.57%\n","221\tTrain Loss: 0.749 | Train Acc: 100.00%\n","\t Val. Loss: 1.444 |  Val. Acc: 28.57%\n","222\tTrain Loss: 0.749 | Train Acc: 100.00%\n","\t Val. Loss: 1.444 |  Val. Acc: 28.57%\n","223\tTrain Loss: 0.748 | Train Acc: 100.00%\n","\t Val. Loss: 1.445 |  Val. Acc: 28.57%\n","224\tTrain Loss: 0.748 | Train Acc: 100.00%\n","\t Val. Loss: 1.446 |  Val. Acc: 28.57%\n","225\tTrain Loss: 0.748 | Train Acc: 100.00%\n","\t Val. Loss: 1.447 |  Val. Acc: 28.57%\n","226\tTrain Loss: 0.749 | Train Acc: 100.00%\n","\t Val. Loss: 1.447 |  Val. Acc: 28.57%\n","227\tTrain Loss: 0.748 | Train Acc: 100.00%\n","\t Val. Loss: 1.446 |  Val. Acc: 28.57%\n","228\tTrain Loss: 0.748 | Train Acc: 100.00%\n","\t Val. Loss: 1.447 |  Val. Acc: 28.57%\n","229\tTrain Loss: 0.748 | Train Acc: 100.00%\n","\t Val. Loss: 1.447 |  Val. Acc: 28.57%\n","230\tTrain Loss: 0.748 | Train Acc: 100.00%\n","\t Val. Loss: 1.448 |  Val. Acc: 28.57%\n","231\tTrain Loss: 0.748 | Train Acc: 100.00%\n","\t Val. Loss: 1.448 |  Val. Acc: 28.57%\n","232\tTrain Loss: 0.748 | Train Acc: 100.00%\n","\t Val. Loss: 1.450 |  Val. Acc: 28.57%\n","233\tTrain Loss: 0.748 | Train Acc: 100.00%\n","\t Val. Loss: 1.451 |  Val. Acc: 28.57%\n","234\tTrain Loss: 0.748 | Train Acc: 100.00%\n","\t Val. Loss: 1.451 |  Val. Acc: 28.57%\n","235\tTrain Loss: 0.748 | Train Acc: 100.00%\n","\t Val. Loss: 1.452 |  Val. Acc: 28.57%\n","236\tTrain Loss: 0.748 | Train Acc: 100.00%\n","\t Val. Loss: 1.453 |  Val. Acc: 28.57%\n","237\tTrain Loss: 0.748 | Train Acc: 100.00%\n","\t Val. Loss: 1.453 |  Val. Acc: 28.57%\n","238\tTrain Loss: 0.748 | Train Acc: 100.00%\n","\t Val. Loss: 1.453 |  Val. Acc: 28.57%\n","239\tTrain Loss: 0.748 | Train Acc: 100.00%\n","\t Val. Loss: 1.453 |  Val. Acc: 28.57%\n","240\tTrain Loss: 0.748 | Train Acc: 100.00%\n","\t Val. Loss: 1.453 |  Val. Acc: 28.57%\n","241\tTrain Loss: 0.748 | Train Acc: 100.00%\n","\t Val. Loss: 1.453 |  Val. Acc: 28.57%\n","242\tTrain Loss: 0.747 | Train Acc: 100.00%\n","\t Val. Loss: 1.453 |  Val. Acc: 28.57%\n","243\tTrain Loss: 0.748 | Train Acc: 100.00%\n","\t Val. Loss: 1.453 |  Val. Acc: 28.57%\n","244\tTrain Loss: 0.748 | Train Acc: 100.00%\n","\t Val. Loss: 1.452 |  Val. Acc: 28.57%\n","245\tTrain Loss: 0.747 | Train Acc: 100.00%\n","\t Val. Loss: 1.453 |  Val. Acc: 28.57%\n","246\tTrain Loss: 0.748 | Train Acc: 100.00%\n","\t Val. Loss: 1.453 |  Val. Acc: 28.57%\n","247\tTrain Loss: 0.747 | Train Acc: 100.00%\n","\t Val. Loss: 1.453 |  Val. Acc: 28.57%\n","248\tTrain Loss: 0.747 | Train Acc: 100.00%\n","\t Val. Loss: 1.454 |  Val. Acc: 28.57%\n","249\tTrain Loss: 0.748 | Train Acc: 100.00%\n","\t Val. Loss: 1.453 |  Val. Acc: 28.57%\n","250\tTrain Loss: 0.747 | Train Acc: 100.00%\n","\t Val. Loss: 1.454 |  Val. Acc: 28.57%\n","251\tTrain Loss: 0.747 | Train Acc: 100.00%\n","\t Val. Loss: 1.454 |  Val. Acc: 28.57%\n","252\tTrain Loss: 0.747 | Train Acc: 100.00%\n","\t Val. Loss: 1.454 |  Val. Acc: 28.57%\n","253\tTrain Loss: 0.747 | Train Acc: 100.00%\n","\t Val. Loss: 1.454 |  Val. Acc: 28.57%\n","254\tTrain Loss: 0.747 | Train Acc: 100.00%\n","\t Val. Loss: 1.454 |  Val. Acc: 28.57%\n","255\tTrain Loss: 0.747 | Train Acc: 100.00%\n","\t Val. Loss: 1.454 |  Val. Acc: 28.57%\n","256\tTrain Loss: 0.747 | Train Acc: 100.00%\n","\t Val. Loss: 1.455 |  Val. Acc: 28.57%\n","257\tTrain Loss: 0.747 | Train Acc: 100.00%\n","\t Val. Loss: 1.455 |  Val. Acc: 28.57%\n","258\tTrain Loss: 0.747 | Train Acc: 100.00%\n","\t Val. Loss: 1.455 |  Val. Acc: 28.57%\n","259\tTrain Loss: 0.747 | Train Acc: 100.00%\n","\t Val. Loss: 1.456 |  Val. Acc: 28.57%\n","260\tTrain Loss: 0.747 | Train Acc: 100.00%\n","\t Val. Loss: 1.456 |  Val. Acc: 28.57%\n","261\tTrain Loss: 0.747 | Train Acc: 100.00%\n","\t Val. Loss: 1.456 |  Val. Acc: 28.57%\n","262\tTrain Loss: 0.747 | Train Acc: 100.00%\n","\t Val. Loss: 1.456 |  Val. Acc: 28.57%\n","263\tTrain Loss: 0.747 | Train Acc: 100.00%\n","\t Val. Loss: 1.456 |  Val. Acc: 28.57%\n","264\tTrain Loss: 0.747 | Train Acc: 100.00%\n","\t Val. Loss: 1.456 |  Val. Acc: 28.57%\n","265\tTrain Loss: 0.747 | Train Acc: 100.00%\n","\t Val. Loss: 1.455 |  Val. Acc: 28.57%\n","266\tTrain Loss: 0.747 | Train Acc: 100.00%\n","\t Val. Loss: 1.454 |  Val. Acc: 28.57%\n","267\tTrain Loss: 0.747 | Train Acc: 100.00%\n","\t Val. Loss: 1.453 |  Val. Acc: 28.57%\n","268\tTrain Loss: 0.747 | Train Acc: 100.00%\n","\t Val. Loss: 1.453 |  Val. Acc: 28.57%\n","269\tTrain Loss: 0.747 | Train Acc: 100.00%\n","\t Val. Loss: 1.453 |  Val. Acc: 28.57%\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-1c90218900fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#save the best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-74e30e9e0b21>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, iterator, criterion)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m#predictions = model(text, text_lengths).squeeze()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m#predictions = model(text, text_lengths)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-c7d5630d11d1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text, text_lengths)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mpacked_embedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mpacked_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_embedded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;31m#hidden = [batch size, num layers * num directions,hid dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m#cell = [batch size, num layers * num directions,hid dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n\u001b[0;32m--> 580\u001b[0;31m                               self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[0m\u001b[1;32m    581\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"JZtuI6NhdFyN"},"source":["# #load weights\n","# path='/content/saved_weights.pt'\n","# model.load_state_dict(torch.load(path));\n","# model.eval();\n","\n","# #inference \n","# import spacy\n","# nlp = spacy.load('en')\n","\n","# def predict(model, sentence):\n","#     tokenized = [tok.text for tok in nlp.tokenizer(sentence)]  #tokenize the sentence \n","#     indexed = [TEXT.vocab.stoi[t] for t in tokenized]          #convert to integer sequence\n","#     length = [len(indexed)]                                    #compute no. of words\n","#     tensor = torch.LongTensor(indexed).to(device)              #convert to tensor\n","#     tensor = tensor.unsqueeze(1).T                             #reshape in form of batch,no. of words\n","#     length_tensor = torch.LongTensor(length)                   #convert to tensor\n","#     prediction = model(tensor, length_tensor)                  #prediction \n","#     return prediction.item()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_aQOajQBdFyP"},"source":["# #make predictions\n","# predict(model, \"Are there any sports that you don't like?\")\n","\n","# #insincere question\n","# predict(model, \"Why Indian girls go crazy about marrying Shri. Rahul Gandhi ji?\")"],"execution_count":null,"outputs":[]}]}